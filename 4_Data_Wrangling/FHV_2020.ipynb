{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JAN EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "jan = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-01 (1).parquet\")\n",
        "jan.head()\n",
        "null_counts = jan.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = jan['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "jan_filtered = jan[~jan['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "jan_filtered = jan_filtered.drop(columns=['airport_fee'])\n",
        "null_counts = jan_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "jan_filtered.shape\n",
        "jan_filtered.info()\n",
        "jan_filtered = jan_filtered.drop(columns=['hvfhs_license_num'])\n",
        "jan_filtered['date'] = jan_filtered['request_datetime'].dt.date\n",
        "jan_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "jan_filtered['date'] = pd.to_datetime(jan_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "jan_filtered['request_datetime'] = pd.to_datetime(jan_filtered['request_datetime']).dt.time\n",
        "jan_filtered['on_scene_datetime'] = pd.to_datetime(jan_filtered['on_scene_datetime']).dt.time\n",
        "jan_filtered['pickup_datetime'] = pd.to_datetime(jan_filtered['pickup_datetime']).dt.time\n",
        "jan_filtered['dropoff_datetime'] = pd.to_datetime(jan_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "jan_filtered['month'] = pd.to_datetime(jan_filtered['date']).dt.month\n",
        "jan_filtered['day'] = pd.to_datetime(jan_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(jan_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(jan_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = jan_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", jan_filtered['month'].unique())\n",
        "\n",
        "jan_filtered = jan_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag','tips', 'tolls'])\n",
        "duplicate_count = jan_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "jan_filtered = jan_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = jan_filtered[jan_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "jan_filtered = jan_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(jan_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = jan_filtered[jan_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "jan_filtered = jan_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(jan_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare','bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=jan_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "jan_new = jan_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    jan_new[column] = winsorize(jan_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    jan_new[column] = np.log1p(jan_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = jan_new[column].quantile(0.25)\n",
        "    Q3 = jan_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    jan_new = jan_new[(jan_new[column] >= (Q1 - 1.5 * IQR)) & (jan_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=jan_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "jan_new['estimated_emissions'] = jan_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "jan_new['emission_levels'] = jan_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(jan_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = jan_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = jan_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = jan_new['dispatching_base_num'].value_counts().head(10).index\n",
        "jan_new = jan_new[jan_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "jan_new.shape\n",
        "jan_new.head()\n",
        "\n",
        "# Save jan_new to a new DataFrame called jan_df\n",
        "jan_df = jan_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of jan_df to verify\n",
        "print(\"Shape of jan_df:\", jan_df.shape)\n",
        "print(jan_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730861613558
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEB EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "feb = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-02.parquet\")\n",
        "feb.head()\n",
        "null_counts = feb.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = feb['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "feb_filtered = feb[~feb['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "feb_filtered = feb_filtered.drop(columns=['airport_fee'])\n",
        "null_counts = feb_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "feb_filtered.shape\n",
        "feb_filtered.info()\n",
        "feb_filtered = feb_filtered.drop(columns=['hvfhs_license_num'])\n",
        "feb_filtered['date'] = feb_filtered['request_datetime'].dt.date\n",
        "feb_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "feb_filtered['date'] = pd.to_datetime(feb_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "feb_filtered['request_datetime'] = pd.to_datetime(feb_filtered['request_datetime']).dt.time\n",
        "feb_filtered['on_scene_datetime'] = pd.to_datetime(feb_filtered['on_scene_datetime']).dt.time\n",
        "feb_filtered['pickup_datetime'] = pd.to_datetime(feb_filtered['pickup_datetime']).dt.time\n",
        "feb_filtered['dropoff_datetime'] = pd.to_datetime(feb_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "feb_filtered['month'] = pd.to_datetime(feb_filtered['date']).dt.month\n",
        "feb_filtered['day'] = pd.to_datetime(feb_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(feb_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(feb_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = feb_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", feb_filtered['month'].unique())\n",
        "\n",
        "feb_filtered = feb_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag','tips', 'tolls'])\n",
        "duplicate_count = feb_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "feb_filtered = feb_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = feb_filtered[feb_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "feb_filtered = feb_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(feb_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = feb_filtered[feb_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "feb_filtered = feb_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(feb_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare','bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=feb_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "feb_new = feb_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    feb_new[column] = winsorize(feb_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    feb_new[column] = np.log1p(feb_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = feb_new[column].quantile(0.25)\n",
        "    Q3 = feb_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    feb_new = feb_new[(feb_new[column] >= (Q1 - 1.5 * IQR)) & (feb_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=feb_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "feb_new['estimated_emissions'] = feb_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "feb_new['emission_levels'] = feb_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(feb_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = feb_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = feb_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = feb_new['dispatching_base_num'].value_counts().head(10).index\n",
        "feb_new = feb_new[feb_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "feb_new.shape\n",
        "feb_new.head()\n",
        "\n",
        "# Save feb_new to a new DataFrame called feb_df\n",
        "feb_df = feb_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of feb_df to verify\n",
        "print(\"Shape of feb_df:\", feb_df.shape)\n",
        "print(feb_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730861999238
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_1 = pd.concat([jan_df, feb_df], ignore_index=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730862007042
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_1.to_parquet('merge_1', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730862032309
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MARCH EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "mar = pd.read_parquet(\"Users/akumar63/input/fhvhv2021.parquet\")\n",
        "mar.head()\n",
        "null_counts = mar.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = mar['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "mar_filtered = mar[~mar['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "mar_filtered = mar_filtered.drop(columns=['airport_fee'])\n",
        "null_counts = mar_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "mar_filtered.shape\n",
        "mar_filtered.info()\n",
        "mar_filtered = mar_filtered.drop(columns=['hvfhs_license_num'])\n",
        "mar_filtered['date'] = mar_filtered['request_datetime'].dt.date\n",
        "mar_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "mar_filtered['date'] = pd.to_datetime(mar_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "mar_filtered['request_datetime'] = pd.to_datetime(mar_filtered['request_datetime']).dt.time\n",
        "mar_filtered['on_scene_datetime'] = pd.to_datetime(mar_filtered['on_scene_datetime']).dt.time\n",
        "mar_filtered['pickup_datetime'] = pd.to_datetime(mar_filtered['pickup_datetime']).dt.time\n",
        "mar_filtered['dropoff_datetime'] = pd.to_datetime(mar_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "mar_filtered['month'] = pd.to_datetime(mar_filtered['date']).dt.month\n",
        "mar_filtered['day'] = pd.to_datetime(mar_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(mar_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(mar_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = mar_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", mar_filtered['month'].unique())\n",
        "\n",
        "mar_filtered = mar_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag','tips', 'tolls'])\n",
        "duplicate_count = mar_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "mar_filtered = mar_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = mar_filtered[mar_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "mar_filtered = mar_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(mar_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = mar_filtered[mar_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "mar_filtered = mar_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(mar_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare','bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=mar_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "mar_new = mar_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    mar_new[column] = winsorize(mar_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    mar_new[column] = np.log1p(mar_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = mar_new[column].quantile(0.25)\n",
        "    Q3 = mar_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    mar_new = mar_new[(mar_new[column] >= (Q1 - 1.5 * IQR)) & (mar_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=mar_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "mar_new['estimated_emissions'] = mar_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "mar_new['emission_levels'] = mar_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(mar_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = mar_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = mar_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = mar_new['dispatching_base_num'].value_counts().head(10).index\n",
        "mar_new = mar_new[mar_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "mar_new.shape\n",
        "mar_new.head()\n",
        "\n",
        "# Save mar_new to a new DataFrame called mar_df\n",
        "mar_df = mar_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of mar_df to verify\n",
        "print(\"Shape of mar_df:\", mar_df.shape)\n",
        "print(mar_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731808803659
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APRIL EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "april = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-04.parquet\")\n",
        "april.head()\n",
        "null_counts = april.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = april['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "april_filtered = april[~april['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "april_filtered = april_filtered.drop(columns=['airport_fee'])\n",
        "null_counts = april_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "april_filtered.shape\n",
        "april_filtered.info()\n",
        "april_filtered = april_filtered.drop(columns=['hvfhs_license_num'])\n",
        "april_filtered['date'] = april_filtered['request_datetime'].dt.date\n",
        "april_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "april_filtered['date'] = pd.to_datetime(april_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "april_filtered['request_datetime'] = pd.to_datetime(april_filtered['request_datetime']).dt.time\n",
        "april_filtered['on_scene_datetime'] = pd.to_datetime(april_filtered['on_scene_datetime']).dt.time\n",
        "april_filtered['pickup_datetime'] = pd.to_datetime(april_filtered['pickup_datetime']).dt.time\n",
        "april_filtered['dropoff_datetime'] = pd.to_datetime(april_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "april_filtered['month'] = pd.to_datetime(april_filtered['date']).dt.month\n",
        "april_filtered['day'] = pd.to_datetime(april_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(april_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(april_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = april_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", april_filtered['month'].unique())\n",
        "\n",
        "april_filtered = april_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag','tips', 'tolls'])\n",
        "duplicate_count = april_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "april_filtered = april_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = april_filtered[april_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "april_filtered = april_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(april_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = april_filtered[april_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "april_filtered = april_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(april_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare','bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=april_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "april_new = april_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    april_new[column] = winsorize(april_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    april_new[column] = np.log1p(april_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = april_new[column].quantile(0.25)\n",
        "    Q3 = april_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    april_new = april_new[(april_new[column] >= (Q1 - 1.5 * IQR)) & (april_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=april_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "april_new['estimated_emissions'] = april_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "april_new['emission_levels'] = april_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(april_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = april_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = april_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = april_new['dispatching_base_num'].value_counts().head(10).index\n",
        "april_new = april_new[april_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "april_new.shape\n",
        "april_new.head()\n",
        "\n",
        "# Save april_new to a new DataFrame called april_df\n",
        "april_df = april_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of april_df to verify\n",
        "print(\"Shape of april_df:\", april_df.shape)\n",
        "print(april_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730862606896
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_2 = pd.concat([mar_df, april_df], ignore_index=True)\n",
        "merge_2.to_parquet('merge_2', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730862620614
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **May EDA**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "may = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-05.parquet\")\n",
        "may.head()\n",
        "null_counts = may.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = may['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "may_filtered = may[~may['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "may_filtered = may_filtered.drop(columns=['airport_fee'])\n",
        "null_counts = may_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "may_filtered.shape\n",
        "may_filtered.info()\n",
        "may_filtered = may_filtered.drop(columns=['hvfhs_license_num'])\n",
        "may_filtered['date'] = may_filtered['request_datetime'].dt.date\n",
        "may_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "may_filtered['date'] = pd.to_datetime(may_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "may_filtered['request_datetime'] = pd.to_datetime(may_filtered['request_datetime']).dt.time\n",
        "may_filtered['on_scene_datetime'] = pd.to_datetime(may_filtered['on_scene_datetime']).dt.time\n",
        "may_filtered['pickup_datetime'] = pd.to_datetime(may_filtered['pickup_datetime']).dt.time\n",
        "may_filtered['dropoff_datetime'] = pd.to_datetime(may_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "may_filtered['month'] = pd.to_datetime(may_filtered['date']).dt.month\n",
        "may_filtered['day'] = pd.to_datetime(may_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(may_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(may_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = may_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", may_filtered['month'].unique())\n",
        "\n",
        "may_filtered = may_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = may_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "may_filtered = may_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = may_filtered[may_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "may_filtered = may_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(may_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = may_filtered[may_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "may_filtered = may_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(may_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=may_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "may_new = may_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    may_new[column] = winsorize(may_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    may_new[column] = np.log1p(may_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = may_new[column].quantile(0.25)\n",
        "    Q3 = may_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    may_new = may_new[(may_new[column] >= (Q1 - 1.5 * IQR)) & (may_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=may_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "may_new['estimated_emissions'] = may_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "may_new['emission_levels'] = may_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(may_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = may_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = may_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = may_new['dispatching_base_num'].value_counts().head(10).index\n",
        "may_new = may_new[may_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "may_new.shape\n",
        "may_new.head()\n",
        "\n",
        "# Save may_new to a new DataFrame called may_df\n",
        "may_df = may_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of may_df to verify\n",
        "print(\"Shape of may_df:\", may_df.shape)\n",
        "print(may_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730862956895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JUNE EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "june = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-06.parquet\")\n",
        "june.head()\n",
        "null_counts = june.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = june['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "june_filtered = june[~june['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "june_filtered = june_filtered.drop(columns=['airport_fee'])\n",
        "june_filtered = june_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = june_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "june_filtered.shape\n",
        "june_filtered.info()\n",
        "june_filtered = june_filtered.drop(columns=['hvfhs_license_num'])\n",
        "june_filtered['date'] = june_filtered['request_datetime'].dt.date\n",
        "june_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "june_filtered['date'] = pd.to_datetime(june_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "june_filtered['request_datetime'] = pd.to_datetime(june_filtered['request_datetime']).dt.time\n",
        "june_filtered['on_scene_datetime'] = pd.to_datetime(june_filtered['on_scene_datetime']).dt.time\n",
        "june_filtered['pickup_datetime'] = pd.to_datetime(june_filtered['pickup_datetime']).dt.time\n",
        "june_filtered['dropoff_datetime'] = pd.to_datetime(june_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "june_filtered['month'] = pd.to_datetime(june_filtered['date']).dt.month\n",
        "june_filtered['day'] = pd.to_datetime(june_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(june_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(june_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = june_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", june_filtered['month'].unique())\n",
        "\n",
        "june_filtered = june_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = june_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "june_filtered = june_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = june_filtered[june_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "june_filtered = june_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(june_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = june_filtered[june_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "june_filtered = june_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(june_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=june_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "june_new = june_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    june_new[column] = winsorize(june_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    june_new[column] = np.log1p(june_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = june_new[column].quantile(0.25)\n",
        "    Q3 = june_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    june_new = june_new[(june_new[column] >= (Q1 - 1.5 * IQR)) & (june_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=june_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "june_new['estimated_emissions'] = june_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "june_new['emission_levels'] = june_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(june_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = june_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = june_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = june_new['dispatching_base_num'].value_counts().head(10).index\n",
        "june_new = june_new[june_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "june_new.shape\n",
        "june_new.head()\n",
        "\n",
        "# Save june_new to a new DataFrame called june_df\n",
        "june_df = june_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of june_df to verify\n",
        "print(\"Shape of june_df:\", june_df.shape)\n",
        "print(june_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730863112445
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_3 = pd.concat([may_df, june_df], ignore_index=True)\n",
        "merge_3.to_parquet('merge_3', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730863124198
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JULY EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "july = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-07.parquet\")\n",
        "july.head()\n",
        "null_counts = july.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = july['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "july_filtered = july[~july['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "july_filtered = july_filtered.drop(columns=['airport_fee'])\n",
        "july_filtered = july_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = july_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "july_filtered.shape\n",
        "july_filtered.info()\n",
        "july_filtered = july_filtered.drop(columns=['hvfhs_license_num'])\n",
        "july_filtered['date'] = july_filtered['request_datetime'].dt.date\n",
        "july_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "july_filtered['date'] = pd.to_datetime(july_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "july_filtered['request_datetime'] = pd.to_datetime(july_filtered['request_datetime']).dt.time\n",
        "july_filtered['on_scene_datetime'] = pd.to_datetime(july_filtered['on_scene_datetime']).dt.time\n",
        "july_filtered['pickup_datetime'] = pd.to_datetime(july_filtered['pickup_datetime']).dt.time\n",
        "july_filtered['dropoff_datetime'] = pd.to_datetime(july_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "july_filtered['month'] = pd.to_datetime(july_filtered['date']).dt.month\n",
        "july_filtered['day'] = pd.to_datetime(july_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(july_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(july_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = july_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", july_filtered['month'].unique())\n",
        "\n",
        "july_filtered = july_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = july_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "july_filtered = july_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = july_filtered[july_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "july_filtered = july_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(july_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = july_filtered[july_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "july_filtered = july_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(july_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=july_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "july_new = july_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    july_new[column] = winsorize(july_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    july_new[column] = np.log1p(july_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = july_new[column].quantile(0.25)\n",
        "    Q3 = july_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    july_new = july_new[(july_new[column] >= (Q1 - 1.5 * IQR)) & (july_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=july_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "july_new['estimated_emissions'] = july_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "july_new['emission_levels'] = july_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(july_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = july_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = july_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = july_new['dispatching_base_num'].value_counts().head(10).index\n",
        "july_new = july_new[july_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "july_new.shape\n",
        "july_new.head()\n",
        "\n",
        "# Save july_new to a new DataFrame called july_df\n",
        "july_df = july_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of july_df to verify\n",
        "print(\"Shape of july_df:\", july_df.shape)\n",
        "print(july_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730864078577
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# August EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "aug = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-08.parquet\")\n",
        "aug.head()\n",
        "null_counts = aug.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = aug['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "aug_filtered = aug[~aug['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "aug_filtered = aug_filtered.drop(columns=['airport_fee'])\n",
        "aug_filtered = aug_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = aug_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "aug_filtered.shape\n",
        "aug_filtered.info()\n",
        "aug_filtered = aug_filtered.drop(columns=['hvfhs_license_num'])\n",
        "aug_filtered['date'] = aug_filtered['request_datetime'].dt.date\n",
        "aug_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "aug_filtered['date'] = pd.to_datetime(aug_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "aug_filtered['request_datetime'] = pd.to_datetime(aug_filtered['request_datetime']).dt.time\n",
        "aug_filtered['on_scene_datetime'] = pd.to_datetime(aug_filtered['on_scene_datetime']).dt.time\n",
        "aug_filtered['pickup_datetime'] = pd.to_datetime(aug_filtered['pickup_datetime']).dt.time\n",
        "aug_filtered['dropoff_datetime'] = pd.to_datetime(aug_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "aug_filtered['month'] = pd.to_datetime(aug_filtered['date']).dt.month\n",
        "aug_filtered['day'] = pd.to_datetime(aug_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(aug_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(aug_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = aug_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", aug_filtered['month'].unique())\n",
        "\n",
        "aug_filtered = aug_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = aug_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "aug_filtered = aug_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = aug_filtered[aug_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "aug_filtered = aug_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(aug_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = aug_filtered[aug_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "aug_filtered = aug_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(aug_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=aug_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "aug_new = aug_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    aug_new[column] = winsorize(aug_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    aug_new[column] = np.log1p(aug_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = aug_new[column].quantile(0.25)\n",
        "    Q3 = aug_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    aug_new = aug_new[(aug_new[column] >= (Q1 - 1.5 * IQR)) & (aug_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=aug_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "aug_new['estimated_emissions'] = aug_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "aug_new['emission_levels'] = aug_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(aug_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = aug_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = aug_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = aug_new['dispatching_base_num'].value_counts().head(10).index\n",
        "aug_new = aug_new[aug_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "aug_new.shape\n",
        "aug_new.head()\n",
        "\n",
        "# Save aug_new to a new DataFrame called aug_df\n",
        "aug_df = aug_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of aug_df to verify\n",
        "print(\"Shape of aug_df:\", aug_df.shape)\n",
        "print(aug_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730864249631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_4 = pd.concat([july_df, aug_df], ignore_index=True)\n",
        "merge_4.to_parquet('merge_4', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730864263398
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEP EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "sep = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-09.parquet\")\n",
        "sep.head()\n",
        "null_counts = sep.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = sep['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "sep_filtered = sep[~sep['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "sep_filtered = sep_filtered.drop(columns=['airport_fee'])\n",
        "sep_filtered = sep_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = sep_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "sep_filtered.shape\n",
        "sep_filtered.info()\n",
        "sep_filtered = sep_filtered.drop(columns=['hvfhs_license_num'])\n",
        "sep_filtered['date'] = sep_filtered['request_datetime'].dt.date\n",
        "sep_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "sep_filtered['date'] = pd.to_datetime(sep_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "sep_filtered['request_datetime'] = pd.to_datetime(sep_filtered['request_datetime']).dt.time\n",
        "sep_filtered['on_scene_datetime'] = pd.to_datetime(sep_filtered['on_scene_datetime']).dt.time\n",
        "sep_filtered['pickup_datetime'] = pd.to_datetime(sep_filtered['pickup_datetime']).dt.time\n",
        "sep_filtered['dropoff_datetime'] = pd.to_datetime(sep_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "sep_filtered['month'] = pd.to_datetime(sep_filtered['date']).dt.month\n",
        "sep_filtered['day'] = pd.to_datetime(sep_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(sep_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(sep_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = sep_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", sep_filtered['month'].unique())\n",
        "\n",
        "sep_filtered = sep_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = sep_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "sep_filtered = sep_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = sep_filtered[sep_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "sep_filtered = sep_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(sep_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = sep_filtered[sep_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "sep_filtered = sep_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(sep_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=sep_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "sep_new = sep_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    sep_new[column] = winsorize(sep_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    sep_new[column] = np.log1p(sep_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = sep_new[column].quantile(0.25)\n",
        "    Q3 = sep_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    sep_new = sep_new[(sep_new[column] >= (Q1 - 1.5 * IQR)) & (sep_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=sep_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "sep_new['estimated_emissions'] = sep_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "sep_new['emission_levels'] = sep_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(sep_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = sep_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = sep_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = sep_new['dispatching_base_num'].value_counts().head(10).index\n",
        "sep_new = sep_new[sep_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "sep_new.shape\n",
        "sep_new.head()\n",
        "\n",
        "# Save sep_new to a new DataFrame called sep_df\n",
        "sep_df = sep_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of sep_df to verify\n",
        "print(\"Shape of sep_df:\", sep_df.shape)\n",
        "print(sep_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730864549560
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCT EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "oct = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-10.parquet\")\n",
        "oct.head()\n",
        "null_counts = oct.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = oct['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "oct_filtered = oct[~oct['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "oct_filtered = oct_filtered.drop(columns=['airport_fee'])\n",
        "oct_filtered = oct_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = oct_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "oct_filtered.shape\n",
        "oct_filtered.info()\n",
        "oct_filtered = oct_filtered.drop(columns=['hvfhs_license_num'])\n",
        "oct_filtered['date'] = oct_filtered['request_datetime'].dt.date\n",
        "oct_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "oct_filtered['date'] = pd.to_datetime(oct_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "oct_filtered['request_datetime'] = pd.to_datetime(oct_filtered['request_datetime']).dt.time\n",
        "oct_filtered['on_scene_datetime'] = pd.to_datetime(oct_filtered['on_scene_datetime']).dt.time\n",
        "oct_filtered['pickup_datetime'] = pd.to_datetime(oct_filtered['pickup_datetime']).dt.time\n",
        "oct_filtered['dropoff_datetime'] = pd.to_datetime(oct_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "oct_filtered['month'] = pd.to_datetime(oct_filtered['date']).dt.month\n",
        "oct_filtered['day'] = pd.to_datetime(oct_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(oct_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(oct_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = oct_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", oct_filtered['month'].unique())\n",
        "\n",
        "oct_filtered = oct_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = oct_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "oct_filtered = oct_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = oct_filtered[oct_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "oct_filtered = oct_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(oct_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = oct_filtered[oct_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "oct_filtered = oct_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(oct_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=oct_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "oct_new = oct_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    oct_new[column] = winsorize(oct_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    oct_new[column] = np.log1p(oct_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = oct_new[column].quantile(0.25)\n",
        "    Q3 = oct_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    oct_new = oct_new[(oct_new[column] >= (Q1 - 1.5 * IQR)) & (oct_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=oct_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "oct_new['estimated_emissions'] = oct_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "oct_new['emission_levels'] = oct_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(oct_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = oct_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = oct_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = oct_new['dispatching_base_num'].value_counts().head(10).index\n",
        "oct_new = oct_new[oct_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "oct_new.shape\n",
        "oct_new.head()\n",
        "\n",
        "# Save oct_new to a new DataFrame called oct_df\n",
        "oct_df = oct_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of oct_df to verify\n",
        "print(\"Shape of oct_df:\", oct_df.shape)\n",
        "print(oct_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730864761589
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_5 = pd.concat([sep_df, oct_df], ignore_index=True)\n",
        "merge_5.to_parquet('merge_5', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730864778113
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nov EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "nov = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-11.parquet\")\n",
        "nov.head()\n",
        "null_counts = nov.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = nov['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "nov_filtered = nov[~nov['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "nov_filtered = nov_filtered.drop(columns=['airport_fee'])\n",
        "nov_filtered = nov_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = nov_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "nov_filtered.shape\n",
        "nov_filtered.info()\n",
        "nov_filtered = nov_filtered.drop(columns=['hvfhs_license_num'])\n",
        "nov_filtered['date'] = nov_filtered['request_datetime'].dt.date\n",
        "nov_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "nov_filtered['date'] = pd.to_datetime(nov_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "nov_filtered['request_datetime'] = pd.to_datetime(nov_filtered['request_datetime']).dt.time\n",
        "nov_filtered['on_scene_datetime'] = pd.to_datetime(nov_filtered['on_scene_datetime']).dt.time\n",
        "nov_filtered['pickup_datetime'] = pd.to_datetime(nov_filtered['pickup_datetime']).dt.time\n",
        "nov_filtered['dropoff_datetime'] = pd.to_datetime(nov_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "nov_filtered['month'] = pd.to_datetime(nov_filtered['date']).dt.month\n",
        "nov_filtered['day'] = pd.to_datetime(nov_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(nov_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(nov_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = nov_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", nov_filtered['month'].unique())\n",
        "\n",
        "nov_filtered = nov_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = nov_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "nov_filtered = nov_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = nov_filtered[nov_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "nov_filtered = nov_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(nov_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = nov_filtered[nov_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "nov_filtered = nov_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(nov_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=nov_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "nov_new = nov_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    nov_new[column] = winsorize(nov_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    nov_new[column] = np.log1p(nov_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = nov_new[column].quantile(0.25)\n",
        "    Q3 = nov_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    nov_new = nov_new[(nov_new[column] >= (Q1 - 1.5 * IQR)) & (nov_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=nov_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "nov_new['estimated_emissions'] = nov_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "nov_new['emission_levels'] = nov_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(nov_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = nov_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = nov_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = nov_new['dispatching_base_num'].value_counts().head(10).index\n",
        "nov_new = nov_new[nov_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "nov_new.shape\n",
        "nov_new.head()\n",
        "\n",
        "# Save nov_new to a new DataFrame called nov_df\n",
        "nov_df = nov_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of nov_df to verify\n",
        "print(\"Shape of nov_df:\", nov_df.shape)\n",
        "print(nov_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730865071284
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dec EDA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "dec = pd.read_parquet(\"Users/aamahesh/fhvhv_tripdata_2020-12.parquet\")\n",
        "dec.head()\n",
        "null_counts = dec.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Display unique values in the 'hvfhs_license_num' column\n",
        "unique_license_nums = dec['hvfhs_license_num'].unique()\n",
        "print(\"Unique values in 'hvfhs_license_num':\")\n",
        "print(unique_license_nums)\n",
        "\n",
        "dec_filtered = dec[~dec['hvfhs_license_num'].isin(['HV0004', 'HV0005'])]\n",
        "dec_filtered = dec_filtered.drop(columns=['airport_fee'])\n",
        "dec_filtered = dec_filtered.dropna(subset=['originating_base_num'])\n",
        "null_counts = dec_filtered.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)\n",
        "dec_filtered.shape\n",
        "dec_filtered.info()\n",
        "dec_filtered = dec_filtered.drop(columns=['hvfhs_license_num'])\n",
        "dec_filtered['date'] = dec_filtered['request_datetime'].dt.date\n",
        "dec_filtered['date'].tail(15)\n",
        "\n",
        "# Extract the date part from 'request_datetime' in mm-dd-yyyy format\n",
        "dec_filtered['date'] = pd.to_datetime(dec_filtered['request_datetime']).dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Remove the date part from the original datetime columns, leaving only the time\n",
        "dec_filtered['request_datetime'] = pd.to_datetime(dec_filtered['request_datetime']).dt.time\n",
        "dec_filtered['on_scene_datetime'] = pd.to_datetime(dec_filtered['on_scene_datetime']).dt.time\n",
        "dec_filtered['pickup_datetime'] = pd.to_datetime(dec_filtered['pickup_datetime']).dt.time\n",
        "dec_filtered['dropoff_datetime'] = pd.to_datetime(dec_filtered['dropoff_datetime']).dt.time\n",
        "\n",
        "# Extract month and day from the 'date' column\n",
        "dec_filtered['month'] = pd.to_datetime(dec_filtered['date']).dt.month\n",
        "dec_filtered['day'] = pd.to_datetime(dec_filtered['date']).dt.day\n",
        "\n",
        "# Check current format of the 'date' column\n",
        "print(\"Current format of 'date' column:\")\n",
        "print(dec_filtered['date'].head())\n",
        "\n",
        "# Display the updated DataFrame to verify changes\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(dec_filtered[['date', 'month', 'day', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']].head())\n",
        "\n",
        "# Get unique values in the 'month' column and count them\n",
        "unique_months = dec_filtered['month'].nunique()\n",
        "print(\"\\nNumber of unique months:\", unique_months)\n",
        "print(\"Unique month values:\", dec_filtered['month'].unique())\n",
        "\n",
        "dec_filtered = dec_filtered.drop(columns=['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_match_flag', 'wav_request_flag', 'tips', 'tolls'])\n",
        "duplicate_count = dec_filtered.duplicated(keep=False).sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "dec_filtered = dec_filtered.drop_duplicates()\n",
        "\n",
        "# Check for duplicates based on specific columns\n",
        "duplicate_columns = ['dispatching_base_num', 'PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = dec_filtered[dec_filtered.duplicated(subset=duplicate_columns, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "dec_filtered = dec_filtered.drop_duplicates(subset=duplicate_columns)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(dec_filtered.shape)\n",
        "\n",
        "# Define a reduced set of columns to check for duplicates\n",
        "duplicate_column = ['PULocationID', 'DOLocationID', 'pickup_datetime', 'dropoff_datetime', 'trip_miles']\n",
        "duplicate_rows = dec_filtered[dec_filtered.duplicated(subset=duplicate_column, keep=False)]\n",
        "duplicate_count = duplicate_rows.shape[0]\n",
        "\n",
        "print(\"Number of duplicate rows based on specified columns:\", duplicate_count)\n",
        "print(\"Duplicate rows based on specified columns:\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "dec_filtered = dec_filtered.drop_duplicates(subset=duplicate_column)\n",
        "print(\"Data after dropping duplicates based on specified columns:\")\n",
        "print(dec_filtered.shape)\n",
        "\n",
        "# Plot boxplots for each numeric column\n",
        "numeric_columns = [\n",
        "    'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', \n",
        "    'base_passenger_fare', 'bcf', 'sales_tax', \n",
        "    'driver_pay'\n",
        "]\n",
        "\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(y=dec_filtered[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()\n",
        "\n",
        "# Winsorization, log transformation, and IQR filtering\n",
        "columns_to_process = ['trip_miles', 'trip_time', 'base_passenger_fare', 'bcf', 'sales_tax', 'driver_pay']\n",
        "dec_new = dec_filtered.copy()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    dec_new[column] = winsorize(dec_new[column], limits=[0.05, 0.05])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    dec_new[column] = np.log1p(dec_new[column])\n",
        "\n",
        "for column in columns_to_process:\n",
        "    Q1 = dec_new[column].quantile(0.25)\n",
        "    Q3 = dec_new[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    dec_new = dec_new[(dec_new[column] >= (Q1 - 1.5 * IQR)) & (dec_new[column] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "for i, column in enumerate(columns_to_process, 1):\n",
        "    plt.subplot(len(columns_to_process), 1, i)\n",
        "    sns.boxplot(y=dec_new[column])\n",
        "    plt.title(f'Boxplot of {column} (after Winsorization, log transformation, and IQR filtering)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate CO₂ emissions\n",
        "emission_factor = 404\n",
        "dec_new['estimated_emissions'] = dec_new['trip_miles'] * emission_factor\n",
        "emission_threshold = 3000\n",
        "dec_new['emission_levels'] = dec_new['estimated_emissions'].apply(lambda x: 1 if x > emission_threshold else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(dec_new[['trip_miles', 'estimated_emissions', 'emission_levels']].head())\n",
        "\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = dec_new.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = dec_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = dec_new['dispatching_base_num'].value_counts().head(10).index\n",
        "dec_new = dec_new[dec_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "dec_new.shape\n",
        "dec_new.head()\n",
        "\n",
        "# Save dec_new to a new DataFrame called dec_df\n",
        "dec_df = dec_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of dec_df to verify\n",
        "print(\"Shape of dec_df:\", dec_df.shape)\n",
        "print(dec_df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730865254880
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_6 = pd.concat([nov_df, dec_df], ignore_index=True)\n",
        "merge_6.to_parquet('merge_6', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730865271454
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Correlation matrix heatmap\n",
        "correlation_matrix = dec_new.corr()\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Filter to include only the top 10 bases\n",
        "base_counts = dec_new['dispatching_base_num'].value_counts()\n",
        "top_10_bases = dec_new['dispatching_base_num'].value_counts().head(10).index\n",
        "dec_new = dec_new[dec_new['dispatching_base_num'].isin(top_10_bases)]\n",
        "dec_new.shape\n",
        "dec_new.head()\n",
        "\n",
        "# Save dec_new to a new DataFrame called dec_df\n",
        "dec_df = dec_new.copy()\n",
        "\n",
        "# Display the shape and first few rows of dec_df to verify\n",
        "print(\"Shape of dec_df:\", dec_df.shape)\n",
        "print(dec_df.head())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}